
**Summary – Metadata Filtering in Retrieval Systems**

**Definition:**

* A retrieval technique that filters documents based on predefined metadata (e.g., title, author, date, access type, region).

**How it Works:**

* Uses rigid, SQL-like criteria to select only documents matching all filter conditions.
* Can filter by one or multiple metadata fields.
* Example: Find all *opinion section* articles by a specific journalist between June–July 2024.

**Usage in RAG Systems:**

* Often used **after** other retrieval methods to narrow down results.
* Filters may be based on **user attributes** (e.g., subscription status, geographic region).
* Example: Paid content excluded for non-subscribers; regional articles shown to users in that region.

**Advantages:**

1. **Simple & intuitive** – easy to implement and debug.
2. **Fast & mature** – well-optimized approach.
3. **Strict control** – can enforce rigid inclusion/exclusion rules.

**Limitations:**

* Not a true search technique — only refines existing results.
* Ignores document content relevance.
* Cannot rank results.
* Too rigid to work as the sole retrieval method.

**Conclusion:**

* Effective for enforcing strict access or scope rules.
* Must be combined with other search methods (e.g., keyword search, semantic search) to ensure relevance and ranking.

Here’s a structured summary with key points of your text on keyword search and TF-IDF in retrieval systems:

---
**Summary – Keyword Search and TF-IDF in Retrieval Systems**

### Overview:

* **Keyword search** retrieves documents based on shared words with the prompt.
* It treats text as a **bag of words** (ignores order, focuses on word presence and frequency).
* Both prompt and documents are represented as **sparse vectors** of word counts.
* These vectors form a **term-document matrix** (or inverted index) to efficiently map words to documents.

### How It Works:

* For each **keyword** in the prompt, documents containing it earn points.
* Simple scoring: 1 point per keyword occurrence per document.
* Documents are ranked by total keyword matches.

### Improvements to Scoring:

1. **Counting multiple occurrences** of keywords in documents improves relevance detection.
2. **Normalization** by document length prevents bias towards longer documents that naturally have more keywords.
3. **Weighting by Inverse Document Frequency (IDF):**

   * Rare words are more informative than common words.
   * IDF = log(total docs / docs containing the word).
   * This downweights common words (e.g., “the”) and upweights rare words (e.g., “pizza”).
4. The matrix with weighted scores is called a **TF-IDF matrix** (Term Frequency-Inverse Document Frequency).

### Benefits of TF-IDF:

* Balances frequency and rarity to better capture document relevance.
* Produces a standard baseline for keyword-based retrieval.
* More relevant documents tend to contain many rare keywords from the prompt.

### Limitations and Next Steps:

* TF-IDF is foundational but can be improved.
* Modern systems often use **BM25**, a refined keyword scoring method.
* Keyword search remains a key component of retrieval augmented generation (RAG) systems but is typically combined with other approaches.

Here’s a concise summary with key points about BM25 and its role in keyword search within retrievers:

---

### Summary – BM25: Improved Keyword Search Algorithm

**What is BM25?**

* BM25 (Best Matching 25) is a widely used keyword search scoring function, evolved from TF-IDF.
* It combines term frequency, document length, and keyword rarity into a relevance score for ranking documents.

**Key Improvements over TF-IDF:**

1. **Term Frequency Saturation:**

   * Diminishing returns on keyword repetition—e.g., a document with "pizza" 20 times isn’t twice as relevant as one with it 10 times.
2. **Document Length Normalization:**

   * Penalizes longer documents less aggressively than TF-IDF, allowing long documents with frequent keywords to score well.
3. **Tunable Hyperparameters:**

   * BM25 has parameters to adjust how quickly saturation and length normalization affect scoring, enabling fine-tuning to specific datasets.

**Why BM25 is Preferred:**

* Generally more effective than TF-IDF in real-world retrieval tasks.
* Similar computational cost but more flexible and accurate.
* Strikes a balance between simplicity and strong performance.

**Strengths of Keyword Search (including BM25):**

* Simple, well-understood, and efficient.
* Ensures retrieved documents contain prompt keywords—important for technical or product-specific queries.
* Often sets a strong baseline for retrieval performance.

**Limitations:**

* Requires exact or close keyword matches.
* Cannot detect relevance if queries and documents share meaning but not exact words.

**Next Step:**

* Semantic search techniques address limitations of keyword search by capturing meaning beyond exact words.



Here’s a clear and concise summary with key points about semantic search from your text:

---

### Summary – Semantic Search

**What is Semantic Search?**

* Matches documents to prompts based on **meaning**, not just exact words.
* Captures nuances that keyword search misses (e.g., “happy” vs “glad”; distinguishes “Python” the language vs snake).

**How It Works:**

* Like keyword search, both documents and prompts are converted into **vectors**.
* But unlike keyword counts, vectors are created using **embedding models**—special mathematical models that map text into a high-dimensional space.

**Embedding Models:**

* Assign vectors to words, sentences, or documents.
* Similar meanings map to **nearby points** in vector space.
* Example: “food” and “cuisine” have close vectors; “trombone” and “cat” are far apart.
* Vectors have many dimensions (hundreds or thousands), capturing complex semantic relationships.

**Measuring Similarity:**

* Distance between vectors quantifies semantic similarity.
* Common distance metrics:

  * **Euclidean distance:** Straight-line distance between points in space.
  * **Cosine similarity:** Measures how aligned two vectors are, ignoring magnitude, ranges from -1 (opposite) to 1 (same direction).
  * **Dot product:** Measures projection length; higher values imply greater similarity.

**Semantic Search Pipeline:**

1. Embed all documents into vector space.
2. Embed user prompt into vector space.
3. Calculate similarity (distance) between prompt vector and each document vector.
4. Rank documents by proximity—closest vectors mean most semantically relevant documents.

**Benefits:**

* Finds relevant documents even if exact keywords differ.
* Captures synonymy, polysemy, and subtle contextual meaning.

**Next Step:**

* Further exploration of embedding model training and how embeddings capture semantics.
Here’s a **concise, structured summary** of that passage with **key points**:

---

## **Embedding Models – How They Work & Are Trained**

### **Core Goal**

* Map **similar text** to vectors that are **close together** in vector space.
* Map **dissimilar text** to vectors that are **far apart**.

---

### **Training Approach**

* **Positive pairs** → similar text (e.g., “good morning” & “hello”) → should be close.
* **Negative pairs** → dissimilar text (e.g., “good morning” & “that’s a noisy trombone”) → should be far.
* Use **massive datasets** with millions of positive & negative pairs.

---

### **Contrastive Training Process**

1. **Initialize**: Model starts with random vectors (no meaning yet).
2. **Evaluate**: Measure how well positive pairs are close & negative pairs are apart.
3. **Update parameters**: Pull positives closer, push negatives apart.
4. **Repeat**: Iterate many times to refine vector positions.
5. **Result**: Semantic meaning emerges — clusters of similar concepts form in high-dimensional space.

---

### **High-Dimensional Space**

* Vectors often have **hundreds or thousands of dimensions** → allows nuanced relationships.
* Each vector is influenced by **many simultaneous push/pull forces** from different pairs.

---

### **Important Properties**

* **Before training** → vectors are random, positions meaningless.
* **After training** → locations gain semantic meaning through clustering.
* **Clusters**: Groups of related concepts (e.g., “lions” cluster vs. “trombones” cluster).
* **Random initialization** means same clusters can appear in different coordinates if retrained.
* **Model-specific**: Only compare vectors from the **same embedding model** (different models → incompatible vectors).

---

### **Practical Use**

* Most developers use **pretrained embedding models** for semantic search or RAG.
* You usually don’t train embeddings or manually calculate distance metrics.
* Understanding training helps in reasoning about retrieval and similarity.

---

Here’s a concise **key-point summary** of your text on **hybrid search in retrievers**:

---

## **Hybrid Search Overview**

* **Goal:** Combine strengths of **metadata filtering**, **keyword search**, and **semantic search** to improve retrieval results.

---

## **Individual Techniques**

1. **Metadata Filtering**

   * Uses rigid yes/no criteria from document metadata.
   * **Strengths:** Fast, simple, strict filtering.
   * **Weakness:** Not effective as sole search method.

2. **Keyword Search**

   * Scores/ranks documents based on exact term matches.
   * **Strengths:** Fast, great for technical terms/product names.
   * **Weakness:** Misses synonyms and conceptually similar content.

3. **Semantic Search**

   * Uses vector embeddings to capture meaning; finds nearest vectors.
   * **Strengths:** Finds meaning-based matches.
   * **Weakness:** Slower, more computationally expensive.

---

## **Hybrid Search Pipeline**

1. **Prompt received** → perform **keyword search** & **semantic search** in parallel.
2. Each returns ranked lists (e.g., 50 docs each).
3. Apply **metadata filters** to both lists to remove irrelevant docs.
4. **Combine rankings** via **Reciprocal Rank Fusion (RRF)**:

   * Score = sum of reciprocals of ranks from each list.
   * **K parameter:** Adjusts how much top rankings dominate.

     * Low K → top ranks dominate.
     * High K → more balanced influence.
   * Ignores the original similarity scores—only uses rank position.
5. **Beta weighting:** Adjusts relative influence of semantic vs keyword ranking in RRF.

   * e.g., β = 0.8 → 80% semantic, 20% keyword.
   * Tune based on whether exact matches or semantic similarity is more important.

---

## **Final Step**

* Return **Top-K** documents from the combined ranking.

---

## **Advantages of Hybrid Search**

* **Keyword search** → precision for exact terms.
* **Semantic search** → captures conceptual similarity.
* **Metadata filtering** → removes irrelevant docs before ranking.
* Flexible tuning via:

  * BM25 parameters
  * Metadata filter rules
  * RRF K and β values

---
Here’s a **concise, structured summary** of that retriever evaluation section:

---

## **Retriever Quality Evaluation**

### **Why Measure?**

* Beyond latency, throughput, or resource usage — the core question is:

  > *“Is it returning relevant documents?”*
* Requires:

  1. **Prompt** (retriever performance can vary per prompt)
  2. **Ranked list** of retrieved documents
  3. **Ground truth** list of relevant documents in the knowledge base

---

### **Core Metrics**

#### 1. **Precision**

* Formula:

  $$
  \text{Precision} = \frac{\text{# relevant docs retrieved}}{\text{# docs retrieved}}
  $$
* Measures **trustworthiness** (penalizes irrelevant results).
* Example: 8 relevant out of 12 retrieved → 66%.

#### 2. **Recall**

* Formula:

  $$
  \text{Recall} = \frac{\text{# relevant docs retrieved}}{\text{# relevant docs in KB}}
  $$
* Measures **comprehensiveness** (penalizes missed relevant docs).
* Example: 8 relevant out of 10 in KB → 80%.
* Often a **trade-off** with precision.

---

### **Precision & Recall at K**

* Evaluates only the **top K** ranked results.
* Example:

  * Precision\@5 = 40% (2/5 relevant)
  * Precision\@10 = 60% (6/10 relevant)
  * Recall\@10 = 75% (6/8 relevant docs found)

---

### **Advanced Ranking Metrics**

#### 3. **Mean Average Precision (MAP)**

* **Average Precision (AP)@K**:

  * Calculate precision at each rank where a relevant doc appears.
  * Average these precisions for that prompt.
* **MAP**:

  * Mean of AP across many prompts.
* Rewards **ranking relevant docs higher**.

#### 4. **Mean Reciprocal Rank (MRR)**

* Reciprocal Rank = $\frac{1}{\text{rank of first relevant doc}}$
* MRR = Average of reciprocal ranks across prompts.
* Emphasizes **finding at least one relevant doc early**.

---

### **Usage & Trade-offs**

* **Recall\@K** → most foundational for retrievers.
* **Precision & MAP** → assess ranking quality & noise level.
* **MRR** → focuses on early-hit relevance.
* Can test **system adjustments** (e.g., keyword vs semantic weighting in hybrid search) and observe metric changes.
* Limitation: All metrics require **ground truth labels**, which are costly to compile.

---

Here’s a **key-point summary** of the passage you shared:

---

## **Scaling Vector Search – Key Notes**

### **1. Naive Vector Search (k-Nearest Neighbors – KNN)**

* **Process:**

  * Create embedding vector for each document and the query.
  * Compute distance from query to every document vector.
  * Sort and return top-K closest vectors.
* **Problem:**

  * **Scales linearly** → search cost = number of documents.
  * Example: 1B docs → 1B distance calculations per query → extremely slow.

---

### **2. Approximate Nearest Neighbors (ANN)**

* **Purpose:** Dramatically speed up vector search by sacrificing *some* accuracy.
* **Approach:** Use **clever data structures** to avoid checking every document.

---

### **3. Navigable Small World (NSW)**

* **Data structure:** Proximity graph → nodes = documents, edges = closest neighbors.
* **Search steps:**

  1. Pick random starting node (candidate).
  2. Check neighbors, move to one closer to query vector.
  3. Repeat until no neighbor is closer.
* **Trade-off:** May miss the *absolute* best match but finds a very close one much faster.

---

### **4. Hierarchical NSW (HNSW)**

* **Extra speed via multi-layer proximity graph:**

  * **Layer 3:** Few vectors (big jumps, fast narrowing).
  * **Layer 2:** More vectors, refined search.
  * **Layer 1:** All vectors, final fine-tuning.
* **Benefits:**

  * Search starts in top layer → big jumps into approximate neighborhood.
  * Runtime ≈ **logarithmic** vs. KNN’s linear cost.
  * Works for **billions** of vectors in **hundreds of milliseconds**.
* **Limitation:** Not guaranteed to return the absolute best match.
* **Precomputation:** Proximity graph building is costly but done ahead of time.

---

### **5. Key Takeaways**

* ANN (especially HNSW) is **essential for scalable** vector retrieval.
* Gains speed by:

  * Avoiding full dataset scans.
  * Using layered graph navigation.
* Trade-off: Slight loss in accuracy, but results are still highly relevant.
* Crucial for **production RAG systems** where retrieval must be both **fast** and **good**.

---

Here’s a concise **key-notes summary** of that passage:

---

## **Key Notes – Vector Databases in RAG Systems**

### **Purpose & Context**

* **Vector databases** store high-dimensional vectors for fast similarity search.
* Crucial for **RAG (Retrieval-Augmented Generation)** systems to handle **semantic search** efficiently.
* Outperform relational databases for tasks like **Approximate Nearest Neighbor (ANN)** searches (e.g., HNSW graph).

### **Common Use Cases**

* Semantic search (dense embeddings).
* Keyword search (sparse vectors).
* Hybrid search (combination of semantic + keyword search).

### **Example: Weaviate**

* Open-source vector database (local or cloud).
* Supports:

  * **Dense vectors** (semantic embeddings).
  * **Sparse vectors** (BM25 keyword search).
  * **Hybrid search** (weighted combination).

### **Core Workflow**

1. **Setup**: Create or connect to a database instance.
2. **Schema Creation**:

   * Define a **collection** (e.g., "article" with `title` and `body` as text).
   * Specify an **embedding model/vectorizer**.
3. **Data Loading**:

   * Batch insert documents (`Batch.addObject`), with error tracking.
4. **Indexing**:

   * Dense embeddings for semantic search.
   * Inverted index for keyword search.
   * ANN index (e.g., HNSW) for vector search.
5. **Searching**:

   * **Vector search**: Returns semantic matches with distance scores.
   * **Keyword search**: Uses BM25 ranking.
   * **Hybrid search**: Combines vector + keyword scores (e.g., α = 0.25 → 25% vector, 75% keyword).
   * **Filtering**: Restrict results based on property values.

### **Best Practices**

* **Hybrid search** is widely used in production for balanced precision & recall.
* Apply filters after retrieval to narrow results.
* Monitor for indexing errors during batch ingestion.

---
You’ve basically pasted a full course segment explaining *why* and *how* chunking is used in vector databases for retrieval-augmented generation (RAG) systems.
Here’s the distilled essence in case you want it in a quick, structured summary:

---

## **Why Chunking Matters**

1. **Embedding model limits** – Models can only handle a certain number of tokens/characters.
2. **Improved search relevance** – Smaller chunks let vectors represent more specific concepts instead of averaging meaning across a large text.
3. **Efficient LLM usage** – Prevents large irrelevant context from filling up the model’s context window.

---

## **Problems Without Chunking**

* A single vector per large document (e.g., a whole book) loses topical precision.
* Retrieval would bring back huge, unfocused text blocks.

---

## **Choosing Chunk Size**

* **Too large** → loses nuance, wastes context space.
* **Too small** → loses surrounding context, weakens semantic meaning.
* **No one-size-fits-all** – balance between context richness and precision.

---

## **Chunking Strategies**

### 1. **Fixed-size chunking**

* Example: 500 characters per chunk.
* Chunks are sequential (1–500, 501–1000, etc.).
* **Overlap**: Keeps partial contexts intact (e.g., 10–20% overlap).

  * Improves relevance.
  * Increases storage cost.

### 2. **Character-based (structural) splitting**

* Split on meaningful boundaries (e.g., newline, paragraph, code function definitions).
* Preserves semantic units better.
* Can produce variable chunk sizes.

---

## **Best Practices**

* Start with **\~500 characters** + **50–100 characters overlap**.
* Inherit metadata for each chunk (e.g., source document, location).
* Adjust method per document type (text, HTML, code).

---

Here’s a concise **key-point summary** of your provided text:

---

## **Chunking in RAG Systems – Key Notes**

### **1. Problem with Simple Chunking**

* **Risk:** Fixed-size or recursive splits may break up context and distort meaning.
* Example: Cutting a sentence mid-way could make it seem the subject *already* achieved something, rather than just dreaming of it.

---

### **2. Advanced Chunking Techniques**

#### **a. Semantic Chunking**

* Groups sentences based on **meaning similarity**.
* Process:

  1. Start with a sentence.
  2. Compare its vector representation to the current chunk’s vector.
  3. If similarity is above threshold → keep in same chunk.
  4. If below threshold → start new chunk.
* **Benefit:** Chunks follow the author’s train of thought.
* **Trade-off:** Computationally expensive (vectorization for every sentence).

#### **b. LLM-Based Chunking**

* Use a language model to split text based on **conceptual boundaries**.
* Can be instructed to:

  * Keep similar concepts together.
  * Split when topics change.
* **Benefit:** Very high performance.
* **Drawback:** Black-box approach, higher cost (though decreasing over time).

---

### **3. Context-Aware Chunking**

* After chunking, use an LLM to **add contextual summaries**.
* Example: If a chunk is a list of names, add a note like “Acknowledgements section”.
* **Benefit:** Improves:

  * Search relevance (better embeddings).
  * Retrieval quality (LLM understands context).
* **Cost:** More preprocessing time and compute, but no search speed impact.

---

### **4. Practical Recommendations**

* **Prototyping:** Start with fixed-size or recursive character splitting.
* **Performance improvement:** Try semantic or LLM-based chunking if justified.
* **Easy upgrade:** Apply context-aware chunking to any method.
* **Always:** Balance cost, complexity, and actual search relevance gains.

---

Here’s a **key-note style summary** of that conversation content:

---

## **RAG Query Parsing – Key Points**

### **1. Why Prompt Cleanup Matters**

* **Problem**: User-written prompts are conversational and messy → bad for vector database search.
* **Solution**: Parse/transform prompts to make them optimal search queries before retrieval.

---

### **2. Core Technique – Query Rewriting**

* **Most common and effective**: Use an LLM to rewrite the prompt.
* **Goal**:

  * Clarify ambiguous phrases.
  * Use domain-specific terminology (e.g., medical terms).
  * Add synonyms to improve match rate.
  * Remove irrelevant details.
* **Example**:

  * **Original**: “I was out walking my dog… shoulder numbness…”
  * **Rewritten**: “Experienced sudden forceful pull on shoulder… possible causes such as neuropathy or nerve impingement.”
* **Benefit**: Significant retrieval quality improvement for minimal extra cost (one LLM call).

---

### **3. Advanced Techniques**

#### **a. Named Entity Recognition (NER)**

* Extracts entities like people, locations, dates, characters.
* Helps improve:

  * Vector search context.
  * Metadata filtering.
* **Example tool**: *Gliner* model – efficient enough to run on every query.

#### **b. Hypothetical Document Embeddings (HyDE)**

* Generate a **hypothetical ideal document** for the query → embed it → use for retrieval.
* Benefits:

  * Makes search “apples to apples” (query vs. documents of similar style).
  * Improves retrieval accuracy.
* Trade-off: Extra latency and compute cost.

---

### **4. Practical Advice**

* Start with **basic query rewriting** → low complexity, high benefit.
* Experiment with NER, HyDE, or other advanced parsing only if your project’s retrieval quality needs further improvement.
* Measure and iterate — not all advanced methods will outperform simple rewriting.

---
Here’s the **key-note style summary** of that section:

---

## **Advanced Semantic Search Architectures – Key Points**

### **1. Vanilla Approach – Bi-Encoder**

* **Process**:

  * Embed each document once (single vector).
  * Embed prompt at query time (single vector).
  * Compare vectors via ANN (approximate nearest neighbors).
* **Pros**:

  * Fast (docs pre-embedded).
  * Low storage (1 vector per document).
* **Cons**:

  * Limited deep contextual matching between prompt & document.

---

### **2. Cross Encoder**

* **Process**:

  * Concatenate prompt + document.
  * Pass combined text into model → outputs **relevance score** (0–1).
  * Captures **rich contextual interactions** missed by bi-encoders.
* **Pros**:

  * Best retrieval quality.
  * Strong performance on relevance metrics.
* **Cons**:

  * **Extremely slow** — must score every doc–prompt pair at query time.
  * No pre-computation possible.
  * Not scalable for millions/billions of docs.
* **Use Case**:

  * Re-ranking a **small subset** of candidates from a faster retriever.

---

### **3. ColBERT (Contextualized Late Interaction over BERT)**

* **Process**:

  * Embed each **token** separately (not whole doc).

    * 1,000-token doc → 1,000 vectors.
  * Prompt tokens also embedded individually.
  * Compare each prompt token to its **most similar** document token.
  * Sum these max similarity scores → document relevance score (**MaxSim**).
* **Pros**:

  * Near cross-encoder quality.
  * Much faster than cross encoder.
  * Still allows pre-embedding documents.
* **Cons**:

  * Large storage — vectors scale with token count.
  * More compute per query than bi-encoder.
* **Use Case**:

  * Precision-critical domains (medical, legal).
  * When quality matters more than storage footprint.

---

### **4. Trade-off Summary**

| Architecture      | Quality   | Speed       | Storage |
| ----------------- | --------- | ----------- | ------- |
| **Bi-Encoder**    | Good      | Fast        | Low     |
| **Cross Encoder** | Best      | Very slow   | N/A     |
| **ColBERT**       | Near-best | Medium-fast | High    |

---

Here’s a concise **key-point summary** of the reranking explanation you shared:

---

## **Reranking in Semantic Search (Key Notes)**

### **Purpose**

* Post-retrieval process to **improve search relevance** after initial vector database results are fetched but before sending them to the LLM.
* Ensures **the most relevant documents** are ultimately returned.

### **How It Works**

1. **Initial Retrieval**

   * Use a vector database (often with hybrid search) to retrieve **more documents than needed** (e.g., 20–100).
2. **Reranking Step**

   * Apply a **high-quality but expensive model** (e.g., cross-encoder or LLM-based scorer) to **re-score and reorder** the retrieved docs.
3. **Final Output**

   * Return only the top-ranked subset (e.g., 5–10 documents).

### **Why Overfetch?**

* Increases the pool for the re-ranker to choose from.
* Helps avoid missing relevant documents that a vector search might rank lower initially.

### **Typical Architectures**

* **Cross-Encoder**

  * Higher accuracy than bi-encoder but slower → feasible only after narrowing down candidates.
* **LLM-Based Reranking**

  * Similar to cross-encoder but uses LLM for scoring document–query pairs.
  * Still costly per document; viable only after initial narrowing.

### **Trade-Off**

* Adds **extra latency** (even if reranking only 20–100 docs) but usually worth it for **better relevance**.

### **Implementation**

* Many vector databases allow reranking with **minimal code changes** (sometimes a single query parameter).
* Recommended first enhancement for improving **RAG** search quality.

### **Best Practice**

* **Overfetch:** 15–25 docs from vector DB.
* **Rerank:** Return top results after re-scoring for **significant relevance boost**.

---

## Module -4


Here’s your **full summary** with **key notes** based on the lecture transcript you shared:

---

## **Full Summary**

The lecture explains **why Retrieval-Augmented Generation (RAG)** works by exploring how **Large Language Models (LLMs)**, specifically those based on the **transformer architecture**, process and understand retrieved information.

The transformer, introduced in *“Attention is All You Need” (2017)*, originally had **two components**:

* **Encoder** – builds a contextual understanding (e.g., for translation from German).
* **Decoder** – generates new text based on that understanding.

Most LLMs use **only the decoder** for text generation, while embedding models use the **encoder** for semantic representation.

When a prompt enters an LLM, it is **tokenized** into discrete units. Each token is:

1. Assigned a **static embedding** (first guess of meaning).
2. Given a **positional vector** to indicate sequence order.

These vectors enter the **attention mechanism**, where each token evaluates relationships with all others. Multiple **attention heads** process different types of relationships (e.g., descriptive vs. spatial). Larger models have more heads, enabling richer contextual understanding.

After attention, the **feedforward network** updates token embeddings into a “second guess,” refined by context. This process repeats for **multiple layers** (8–64 times in large models), progressively improving understanding.

Once embeddings are refined, the model predicts the next token via a **probability distribution** over its vocabulary. One token is sampled, added to the sequence, and the entire process repeats for each new token. This ensures generated text maintains coherence with both the original and newly generated tokens.

From a RAG perspective:

* The attention mechanism allows LLMs to integrate retrieved documents meaningfully into their generation process.
* LLM outputs are **probabilistic**, so grounding and controlling randomness is crucial.
* Transformer operations are **computationally expensive**, with costs increasing as prompt length grows.

---

## **Key Notes**

### **Transformer Basics**

* **Introduced**: 2017 (*Attention is All You Need*).
* **Components**: Encoder (context understanding), Decoder (generation).
* LLMs = usually **decoder-only**; embedding models = encoder.

### **Processing Steps**

1. **Tokenization** – split text into tokens.
2. **Static embeddings** – same vector every time for same token.
3. **Positional vectors** – encode order in sequence.
4. **Attention mechanism** – tokens evaluate relationships with all others.
5. **Multiple attention heads** – capture different types of relationships.
6. **Feedforward network** – update embeddings with context-based meaning.
7. Repeat through multiple **layers** (8–64).
8. **Next token prediction** – probability distribution → sampling → repetition.

### **RAG Insights**

* **Why RAG works**: Attention integrates retrieved info into generation.
* **Randomness**: Even with relevant info, LLM may not use it → need grounding.
* **Cost factor**: Expensive computation due to all-tokens-to-all-tokens attention.

---











